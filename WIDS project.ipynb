{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Women in Data Science Datathon Project\n",
    "The Women in Data Science Datathon is inspired by research to help disadvantaged women participate more fully in their local economies.  The dataset is created from a survey in India regarding access to financial tools and digital technology.  In this Kaggle competition, we will predict the gender of the survey respondents based on their answers.  We will experiment with different ways of cleaning the data and various classification models in order to get the highest accuracy score.  While this iPython notebook ends at this step, we will then use our best model to predict the test data and then submit the results to Kaggle to be scored online."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Data\n",
    "After making our imports, we will clean the data.  First, we will drop columns with all empty values.  Next, we need to separate the columns into categoric and numeric factors.  For the categoric factors, we will replace empty values with 0 to create a null group and then create dummy/indicator variables.  For the numeric factors, we will replace empty values and non-answers with the median value of the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:/train.csv\")\n",
    "Y = df['is_female']\n",
    "X = df.drop(['is_female','train_id'], axis = 1)\n",
    "X = X.dropna(axis=1, how='all') # If all values are nans, drop col\n",
    "\n",
    "categoricColumns = X.columns.tolist()\n",
    "numericColumns = []\n",
    "\n",
    "for i in categoricColumns:\n",
    "    if (\"MT17_\" in i or \"FF16_\" in i or \"MM5_\" in i or \"FF9\" in i\n",
    "         or \"MM8_\" in i or \"MM9_\" in i or \"MM17_\" in i or \"MM32_\" in i\n",
    "         or \"MM42_\" in i or \"MMP2_\" in i or \"MMP4_\" in i or \"IFI4_\" in i\n",
    "         or \"IFI2_\" in i or \"IFI14_\" in i or \"IFI15_\" in i or \"IFI17_\" in i\n",
    "         or \"FL3\" in i or \"FL8_\" in i or \"LN1A\" in i or \"LN1B\" in i\n",
    "         or \"LN2_1\" in i or \"LN2_2\" in i or \"LN2_3\" in i or \"LN2_4\" in i):\n",
    "        numericColumns.append(i)\n",
    "        categoricColumns.remove(i)\n",
    "\n",
    "for i in numericColumns:\n",
    "    median = X[X[i].between(1, 20, inclusive=True)][i].median()\n",
    "    X[i] = X[i].fillna(median) \n",
    "    X[i].replace([96, 99], median)\n",
    "\n",
    "for i in categoricColumns:\n",
    "    X[i] = X[i].fillna(0) \n",
    "    X = pd.get_dummies(X, columns=[i])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Models\n",
    "Now the data has been cleaned, we will split our data into an 80/20 training and testing set.  Next, we will run four classification models: Logistic Regression, Random Forest, Gradient Boosting, and AdaBoost with decision tree classifier.  Logistic Regression and Random Forest gives us a baseline for what accuracies we can expect.  However, we expect to have the best results with our ensemble models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: 0.761904761905\n",
      "Random Forest: 0.833333333333\n",
      "Gradient Boosting: 0.857142857143\n",
      "AdaBoost: 0.904761904762\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# Logistic Regression\n",
    "LogReg = linear_model.LogisticRegression()\n",
    "LogReg.fit(X_train, y_train)\n",
    "y_pred = LogReg.predict(X_test)\n",
    "print('Logistic Regression: '+str(accuracy_score(y_test, y_pred)))\n",
    "\n",
    "# Random Forest\n",
    "clf = RandomForestClassifier(n_jobs=2, random_state=1)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print('Random Forest: '+str(accuracy_score(y_test, y_pred)))\n",
    "\n",
    "# Gradient Boosting\n",
    "clf = GradientBoostingClassifier(n_estimators=5000,\n",
    "                                 learning_rate=0.5,\n",
    "                                 max_depth=None, \n",
    "                                 random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print('Gradient Boosting: '+str(accuracy_score(y_test, y_pred)))\n",
    "\n",
    "# AdaBoost\n",
    "clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=3),\n",
    "                         algorithm=\"SAMME\",\n",
    "                         n_estimators=1000,\n",
    "                         learning_rate=1)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print('AdaBoost: '+str(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Our results were close to what we expected with our ensemble classifiers outperforming our simple classifiers with AdaBoost being our best model.  Next, we used AdaBoost to produce the predictions that we submitted to Kaggle for the competition.  The preliminary results have us at 91.444% accuracy.  However, this score only considers 16% of the testing set and the final results will be released after the competition is over.  Somethings that may improve our model include cleaning the data differently, trying other models and tuning parameters in our models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
